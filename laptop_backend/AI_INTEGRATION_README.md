# AI Layer Integration - Snapdragon NPU Backend

## ğŸš€ Quick Start

This backend now supports **NPU-accelerated LLM inference** using Qualcomm's Snapdragon Hexagon processor.

### Directory Structure

```
laptop_backend/
â”œâ”€â”€ ai/                          # AI Layer (NEW)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inference/              # NPU inference engine
â”‚   â”‚   â”œâ”€â”€ gemini_engine.py   # Core Gemini inference logic
â”‚   â”‚   â””â”€â”€ cache_manager.py   # KV cache management
â”‚   â”œâ”€â”€ prompts/                # Structured prompts
â”‚   â”‚   â”œâ”€â”€ plan_prompts.py    # Diet/workout generation
â”‚   â”‚   â”œâ”€â”€ nutrition_prompts.py
â”‚   â”‚   â””â”€â”€ health_prompts.py
â”‚   â””â”€â”€ utils/                  # Helper utilities
â”‚       â””â”€â”€ model_utils.py
â”œâ”€â”€ routes/                      # Flask routes
â”‚   â”œâ”€â”€ plan_ai.py             # AI-enhanced plan routes (NEW)
â”‚   â”œâ”€â”€ user_ai.py             # AI-enhanced user routes (NEW)
â”‚   â”œâ”€â”€ plan.py                # Original routes
â”‚   â”œâ”€â”€ user.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ services/                    # Business logic
â”œâ”€â”€ models/                      # Model artifacts (NEW)
â”‚   â”œâ”€â”€ llama3_8b_onnx/        # Exported ONNX model
â”‚   â””â”€â”€ llama3_htp_context/    # Compiled HTP binary
â”œâ”€â”€ setup_npu.sh                # Setup script (NEW)
â”œâ”€â”€ NPU_INTEGRATION_GUIDE.md   # Detailed guide (NEW)
â””â”€â”€ requirements.txt            # Updated dependencies
```

---

## ğŸ“‹ Prerequisites

- **Python:** 3.10 or 3.11
- **Hardware:** Snapdragon device with Hexagon NPU (8 Gen 2+, X Elite)
- **QNN SDK:** Download from [Qualcomm Developer Network](https://developer.qualcomm.com)
- **Storage:** ~10GB for model artifacts

---

## ğŸ”§ Installation

### Option 1: Automated Setup (Recommended)

```bash
# Clone repository
cd laptop_backend

# Run setup script
./setup_npu.sh

# This will:
# - Create virtual environment
# - Install dependencies
# - Create model directories
# - Generate .env.qnn configuration
```

### Option 2: Manual Setup

```bash
# Create virtual environment
python3.10 -m venv venv_qnn
source venv_qnn/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create model directories
mkdir -p models/llama3_8b_onnx models/llama3_htp_context

# Copy and configure environment
cp .env.qnn.example .env.qnn
nano .env.qnn  # Edit paths
```

---

## ğŸ“¦ Model Preparation

### Step 1: Export Model to ONNX

```bash
source venv_qnn/bin/activate

python - <<EOF
from optimum.onnxruntime import ORTModelForCausalLM
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
onnx_path = "./models/llama3_8b_onnx"

print("Exporting model to ONNX...")
model = ORTModelForCausalLM.from_pretrained(
    model_id,
    export=True,
    use_merged=True
)
model.save_pretrained(onnx_path)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.save_pretrained(onnx_path)
print("âœ“ Export complete!")
EOF
```

### Step 2: Quantize and Compile for NPU

See detailed instructions in [`NPU_INTEGRATION_GUIDE.md`](./NPU_INTEGRATION_GUIDE.md#model-preparation)

**Quick version:**
```bash
# Set QNN SDK path
export QNN_SDK_ROOT="/opt/qcom/aistack/qairt/2.26.0"
export PATH="${QNN_SDK_ROOT}/bin:${PATH}"

# Convert to QNN format
qnn-onnx-converter \
    --input_network ./models/llama3_8b_onnx/decoder_model_merged.onnx \
    --output_path ./models/llama3_qnn.cpp \
    --quantization_overrides "weights:4" \
    --act_bitwidth 16

# Compile to HTP binary
qnn-context-binary-generator \
    --backend ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtp.so \
    --model ./models/llama3_qnn.cpp \
    --output_dir ./models/llama3_htp_context \
    --binary_file llama3_8b_w4a16.bin
```

---

## ğŸ¯ API Endpoints

### Standard Endpoints (No AI)

```bash
POST /users/<user_id>/plan              # Create plan (mock data)
GET  /users/<user_id>/health            # Get health profile
PUT  /users/<user_id>/nutrition         # Update nutrition profile
```

### AI-Enhanced Endpoints (NPU-Accelerated)

```bash
POST /ai/users/<user_id>/plan                    # AI-generated plan
GET  /ai/users/<user_id>/health/analyze          # AI health analysis
PUT  /ai/users/<user_id>/nutrition               # AI nutrition recommendations
POST /ai/users/<user_id>/plan/validate           # Validate existing plan
PUT  /ai/users/<user_id>/plan/adjust             # Adjust plan with feedback
GET  /ai/health                                   # NPU engine health check
```

---

## ğŸ’¡ Usage Examples

### 1. Generate AI Diet Plan

```bash
curl -X POST http://localhost:5000/ai/users/user123/plan \
  -H "Content-Type: application/json" \
  -d '{
    "plan_type": "diet",
    "duration_weeks": 4,
    "intensity": "moderate",
    "specific_goals": ["weight_loss"],
    "use_ai": true
  }'
```

**Response:**
```json
{
  "message": "Plan created successfully",
  "plan": {
    "ai_generated": true,
    "generation_method": "npu_llm",
    "diet": [
      {
        "weekName": "Week 1",
        "meals": {
          "breakfast": {
            "name": "Protein Oatmeal Bowl",
            "calories": 380,
            "protein": 25,
            "carbs": 52,
            "fats": 12,
            "completed": false
          },
          ...
        }
      }
    ]
  }
}
```

### 2. Get Health Analysis

```bash
curl http://localhost:5000/ai/users/user123/health/analyze
```

**Response:**
```json
{
  "analysis": {
    "bmi_assessment": {
      "category": "Normal",
      "description": "Your BMI of 22.5 is within the healthy range"
    },
    "health_insights": [
      "Your activity level is appropriate for your fitness goals",
      "Consider increasing protein intake for muscle development"
    ],
    "recommendations": [
      "Maintain current weight through balanced nutrition",
      "Add 2-3 strength training sessions per week"
    ]
  }
}
```

### 3. Check NPU Engine Status

```bash
curl http://localhost:5000/ai/health
```

**Response:**
```json
{
  "status": "healthy",
  "engine": "NPU (Qualcomm HTP)",
  "mode": "production",
  "model_loaded": true,
  "tokenizer_loaded": true,
  "test_inference": "passed",
  "kv_cache_stats": {
    "cache_size_mb": 145.23,
    "hit_rate": "67.5%",
    "hits": 1234,
    "misses": 589
  }
}
```

---

## ğŸ¨ Code Examples

### Using NPU Engine Directly

```python
from ai import get_gemini_engine

# Get singleton engine instance
engine = get_gemini_engine()

# Generate text
response = engine.generate(
    prompt="Create a healthy breakfast recipe with oatmeal",
    max_new_tokens=200,
    temperature=0.7,
    use_cache=True
)

print(response)

# Check engine info
info = engine.get_model_info()
print(f"Running on: {info['backend']}")
print(f"Quantization: {info['quantization']}")

# Cache statistics
stats = engine.kv_cache.get_stats()
print(f"Cache hit rate: {stats['hit_rate']}")
```

### Using Prompt Templates

```python
from ai.prompts import plan_prompts
from ai import get_gemini_engine

# Prepare user data
user_context = {
    "profile": {
        "age": 28,
        "weight": 75,
        "height": 180,
        "fitness_goal": "muscle_gain"
    },
    "nutrition": {
        "diet_type": "balanced",
        "calorie_goal": 2500
    },
    "preferences": {
        "duration_weeks": 4,
        "intensity": "high"
    }
}

# Generate structured prompt
prompt = plan_prompts.generate_plan_prompt(user_context, "workout")

# Get AI response
engine = get_gemini_engine()
plan = engine.generate(prompt, max_new_tokens=1000)
```

---

## âš™ï¸ Configuration

### Environment Variables (.env.qnn)

```bash
# Model Configuration
QNN_MODEL_CONTEXT=./models/llama3_htp_context/llama3_8b_w4a16.bin
MAX_CONTEXT_LENGTH=2048
MAX_NEW_TOKENS=512

# Generation Parameters
TEMPERATURE=0.7          # Creativity (0.1-2.0)
TOP_P=0.9               # Nucleus sampling (0.1-1.0)
KV_CACHE_SIZE_MB=512    # Cache size

# Performance Tuning
HTP_PERFORMANCE_MODE=burst              # burst|balanced|sustained
HTP_GRAPH_FINALIZATION_OPTIMIZATION_MODE=3
```

### Performance Modes

| Mode | Performance | Power | Use Case |
|------|------------|-------|----------|
| `burst` | Highest | High | Short inference tasks |
| `balanced` | Good | Medium | General use |
| `sustained` | Steady | Lower | Long-running tasks |

---

## ğŸ› Troubleshooting

### Issue: NPU Engine Fails to Initialize

**Symptoms:** `QNN EP not active` warning

**Solution:**
```bash
# Check QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/lib/aarch64-android/libQnnHtp.so

# Verify LD_LIBRARY_PATH
export LD_LIBRARY_PATH="${QNN_SDK_ROOT}/lib/aarch64-android:${LD_LIBRARY_PATH}"
```

### Issue: Running in Simulation Mode

**Symptoms:** "Running in SIMULATION mode" message

**Causes:**
- Model binary not found
- Running on non-ARM64 architecture
- QNN libraries not accessible

**Solution:** Engine will use mock responses. To enable NPU:
1. Ensure running on Snapdragon device
2. Complete model preparation steps
3. Verify `QNN_MODEL_CONTEXT` path in `.env.qnn`

### Issue: Out of Memory

**Solution:** Reduce context length and cache size
```bash
MAX_CONTEXT_LENGTH=1024
MAX_NEW_TOKENS=256
KV_CACHE_SIZE_MB=256
```

---

## ğŸ“Š Performance Benchmarks

### Snapdragon 8 Gen 3

| Metric | Value |
|--------|-------|
| First token latency | ~250ms |
| Token generation speed | 35-45 tokens/sec |
| Memory usage (8B W4A16) | ~5.5GB |
| Cache hit rate (typical) | 60-70% |

### Expected Response Times

| Task | Tokens | Time |
|------|--------|------|
| Diet plan generation | ~800 | 18-25s |
| Health analysis | ~400 | 9-12s |
| Meal suggestion | ~200 | 5-7s |

---

## ğŸ”„ Migration Guide

### From Original Routes to AI Routes

**Before:**
```python
# Original route (mock data)
POST /users/<user_id>/plan
```

**After:**
```python
# AI-enhanced route (NPU-powered)
POST /ai/users/<user_id>/plan
```

**Backward Compatibility:** Original routes remain functional. Add `/ai/` prefix to use NPU acceleration.

---

## ğŸ“š Additional Resources

- **[NPU_INTEGRATION_GUIDE.md](./NPU_INTEGRATION_GUIDE.md)** - Comprehensive technical guide
- **[Qualcomm AI Stack Docs](https://developer.qualcomm.com/software/qualcomm-ai-stack)** - Official documentation
- **[ONNX Runtime QNN EP](https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html)** - ONNX Runtime guide

---

## ğŸ¤ Contributing

When adding new AI features:

1. Add prompt templates to `ai/prompts/`
2. Create route handlers in `routes/*_ai.py`
3. Test with both NPU and simulation modes
4. Update documentation

---

## ğŸ“ License

This project is part of the Snapdragon Hackathon submission.

---

## ğŸ™‹ Support

For issues or questions:
1. Check [Troubleshooting](#troubleshooting) section
2. Review [NPU_INTEGRATION_GUIDE.md](./NPU_INTEGRATION_GUIDE.md)
3. File an issue on GitHub

---

**Built with â¤ï¸ for Snapdragon devices**
